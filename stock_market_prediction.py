# -*- coding: utf-8 -*-
"""Stock_Market_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NIEXrd70HXG3SSXErbbaKmg6paX0ppDt

## **Stock Market Price Prediction Analysis**

### **1. Business Objective**

Stock price prediction is a crucial task in financial markets that enables investors, traders and financial institutions to make informed decisions about buying, selling or holding securities. Accurate stock price forecasting can provide significant competitive advantages in the market.

The primary objectives of this analysis are:

- **Investment Decision Support**: Provide data-driven insights to help investors optimise their portfolio allocation and timing of trades
- **Risk Management**: Identify potential market trends and volatility patterns to minimize investment risks  
- **Market Trend Analysis**: Understand historical price movements and patterns to predict future market behavior
- **Algorithmic Trading Support**: Develop predictive models that can be integrated into automated trading systems

Since we have a time series prediction problem, we will apply machine learning techniques specifically Long Short-Term Memory (LSTM) neural networks to create accurate stock price predictions based on historical data patterns.

### **2. Data Collection**

This analysis uses real-time stock market data for Google (GOOG) spanning the last 20 years, obtained through the Yahoo Finance API. The dataset includes daily trading information including opening prices, closing prices, high/low values and trading volumes.
"""

# Importing necessary libraries for data collection
import yfinance as yf

from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# Setting up plotting parameters
pd.set_option('display.max_rows', 500)
pd.options.display.max_columns = None
pd.options.display.float_format = '{:.4f}'.format
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (25, 5)

# %matplotlib inline
matplotlib.rcParams["figure.figsize"] = (25, 5)

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization
from keras.regularizers import l2
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

import joblib

# Setting up data collection parameters
end = datetime.now()
start = datetime(end.year-20, end.month, end.day)
stock = "GOOG"

print(f"Collecting stock data for {stock} from {start.date()} to {end.date()}")
# Downloading Google stock data
google_data = yf.download(stock, start, end)

"""## **3. Data Preparation/Preprocessing**

We will start with loading and familiarising ourselves with the dataset so that we can prepare the data for Machine Learning (ML) modeling.
"""

# Displaying information about the dataset
print(f"Number of records: {google_data.shape[0]}")
print(f"Number of features: {google_data.shape[1]}")
print(f"Features: {list(google_data.columns)}")
print(f"Date range: {google_data.index.min().date()} to {google_data.index.max().date()}")
google_data.head()

"""#### **3.1 Data Quality Assessment**

Before proceeding with analysis, we need to assess the quality of our data by checking for missing values, data types, and basic statistical properties.

"""

# Checking data types and structure
print("Data type and info: ")
google_data.info()

# Checking for missing values
missing_values = google_data.isnull().sum()
print("Missing values per column:")
for col, missing in missing_values.items():
    print(f"{col}: {missing} ({missing/len(google_data)*100:.2f}%)")
total_missing = missing_values.sum()
print(f"Total missing values: {total_missing}")

google_data.describe()

"""##### Observations:

The dataset contains daily trading data with no missing values, indicating high data quality. All features are numerical (float64) representing different price points and trading volume.

### **3.2 Exploratory Data Analysis**

After the data quality assessment, we can now perform EDA on the dataset to discover patterns and relationships that will help in understanding the stock's behavior better.

##### **3.2.1 Price Trend Analysis**

Analyzing the overall price trends and patterns over the 20-year period.
"""

def plot_stock_data(data, columns, title, ylabel="Price ($)"):
    """
    Function to create stock data visualizations
    """
    plt.figure(figsize=(25, 8))

    if isinstance(columns, list):
        for col in columns:
            if isinstance(col, tuple):  # Handle MultiIndex columns
                plt.plot(data.index, data[col], label=col[0], linewidth=2)
            else:
                plt.plot(data.index, data[col], label=col, linewidth=2)
        plt.legend()
    else:
        if isinstance(columns, tuple):  # Handle MultiIndex columns
            plt.plot(data.index, data[columns], linewidth=2)
        else:
            plt.plot(data.index, data[columns], linewidth=2)

    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel("Date", fontsize=10)
    plt.ylabel(ylabel, fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# Analysing closing price trends
close_col = ('Close', 'GOOG') if ('Close', 'GOOG') in google_data.columns else 'Close'
plot_stock_data(google_data, close_col, "Google (GOOG) Stock Closing Price - 20 Year Trend")

# Analysing all price components
price_columns = ['Open', 'High', 'Low', 'Close']

# Handle potential MultiIndex columns
actual_columns = []
for col in price_columns:
    if (col, 'GOOG') in google_data.columns:
        actual_columns.append((col, 'GOOG'))
    elif col in google_data.columns:
        actual_columns.append(col)

if actual_columns:
    plot_stock_data(google_data, actual_columns, "Google Stock - All Price Components")

"""#### **3.2.2 Moving Averages Analysis**

Moving averages are crucial technical indicators that help smooth out price data and identify trends. We'll calculate and visualize different moving average periods.

"""

# Extracting the close price for analysis (handling MultiIndex if present)
if ('Close', 'GOOG') in google_data.columns:
    close_prices = google_data[('Close', 'GOOG')]
else:
    close_prices = google_data['Close']

# Calculate different moving averages
google_data['MA_50_days'] = close_prices.rolling(window=50).mean()
google_data['MA_100_days'] = close_prices.rolling(window=100).mean()
google_data['MA_200_days'] = close_prices.rolling(window=200).mean()

# Plot closing price with moving averages
plt.figure(figsize=(25, 8))
plt.plot(google_data.index, close_prices, label='Close Price', linewidth=1, alpha=0.7)
plt.plot(google_data.index, google_data['MA_50_days'], label='50-Day MA', linewidth=2)
plt.plot(google_data.index, google_data['MA_100_days'], label='100-Day MA', linewidth=2)
plt.plot(google_data.index, google_data['MA_200_days'], label='200-Day MA', linewidth=2)
plt.title('Google Stock Price with Moving Averages', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Price ($)', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""### **3.2.3 Price Change and Volatility Analysis**

Understanding price volatility and daily changes is essential for risk assessment and prediction accuracy.
"""

google_data['Daily_Return'] = close_prices.pct_change()
google_data['Price_Change'] = close_prices.diff()

# Display basic statistics for returns
print("Daily Returns Statistics:")
returns_stats = google_data['Daily_Return'].describe()
print(returns_stats)

# Plot daily returns distribution
plt.figure(figsize=(25, 5))
plt.subplot(1, 2, 1)
plt.hist(google_data['Daily_Return'].dropna(), bins=50, alpha=0.7, edgecolor='black')
plt.title('Daily Returns Distribution')
plt.xlabel('Daily Return (%)')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.subplot(1, 2, 2)
plt.plot(google_data.index, google_data['Daily_Return'], alpha=0.7, linewidth=0.8)
plt.title('Daily Returns Over Time')
plt.xlabel('Date')
plt.ylabel('Daily Return (%)')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Calculate rolling volatility
google_data['Volatility_30d'] = google_data['Daily_Return'].rolling(window=30).std()
plot_stock_data(google_data, 'Volatility_30d', "30-Day Rolling Volatility", "Volatility")

"""### **3.2.4 Yearly Trading Activity Analysis**

Analyzing trading patterns across different years to understand market behavior and data distribution.
"""

# Count records per year
yearly_counts = {}

print("Records per year:")
for year in range(2004, 2025):
    count = sum(1 for date in google_data.index if date.year == year)
    if count > 0:
        yearly_counts[year] = count
        print(f"{year}: {count} trading days")

# Plot yearly activity
years = list(yearly_counts.keys())
counts = list(yearly_counts.values())

plt.figure(figsize=(25, 5))
plt.bar(years, counts, alpha=0.7, edgecolor='black')
plt.title('Trading Days Per Year', fontsize=16, fontweight='bold')
plt.xlabel('Year', fontsize=12)
plt.ylabel('Number of Trading Days', fontsize=12)
plt.grid(True, alpha=0.3, axis='y')
plt.xticks(years, rotation=45)
plt.tight_layout()
plt.show()

"""### **4. Feature Engineering and Data Preprocessing**

#### **4.1 Data Scaling and Normalization**

For LSTM models, we need to scale our data to improve model performance and convergence.
"""

# Prepare data for scaling - using only Close prices for prediction
price_data = close_prices.values.reshape(-1, 1)
print(f"Original price range: ${price_data.min():.2f} - ${price_data.max():.2f}")

# Initialize and fit scaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(price_data)

print(f"Scaled data range: {scaled_data.min():.4f} - {scaled_data.max():.4f}")
print(f"Total data points for model training: {len(scaled_data)}")

"""#### **4.2 Time Series Data Preparation**

Creating sequences for LSTM model training. We'll use a lookback window to predict the next day's price.
"""

# Parameters for sequence creation
lookback_window = 100  # Use 100 days of historical data to predict next day

# Create sequences
x_data = []
y_data = []
for i in range(lookback_window, len(scaled_data)):
    x_data.append(scaled_data[i-lookback_window:i])  # Previous 100 days
    y_data.append(scaled_data[i])                    # Next day target

# Convert to numpy arrays
x_data, y_data = np.array(x_data), np.array(y_data)
print(f"Features shape: {x_data.shape}")
print(f"Targets shape: {y_data.shape}")
print(f"Total sequences created: {len(x_data)}")

"""### **4.3 Train-Test Split**

Splitting the data chronologically to maintain the temporal order crucial for time series analysis.
"""

# Calculate split point (70% for training, 30% for testing)
split_ratio = 0.7
splitting_len = int(len(x_data) * split_ratio)

# Split the data
x_train = x_data[:splitting_len]
y_train = y_data[:splitting_len]
x_test = x_data[splitting_len:]
y_test = y_data[splitting_len:]
print(f"Traimig set:")
print(f"  - Features: {x_train.shape}")
print(f"  - Targets: {y_train.shape}")
print(f"  - Time period: {google_data.index[lookback_window]} to {google_data.index[splitting_len + lookback_window - 1]}")

print(f"Test set:")
print(f"  - Features: {x_test.shape}")
print(f"  - Targets: {y_test.shape}")
print(f"  - Time period: {google_data.index[splitting_len + lookback_window]} to {google_data.index[-1]}")
print(f"  - Data split ratio: {split_ratio*100:.0f}% training, {(1-split_ratio)*100:.0f}% testing")

"""## **5. Model Building**

### **5.1 LSTM Model Architecture**

Building a sophisticated LSTM model with regularization techniques to prevent overfitting and improve generalization.
"""

# Model architecture parameters
lstm1_units = 128
lstm2_units = 64
dense1_units = 50
dense2_units = 25
dropout_rate = 0.3
l2_reg = 0.01

# Build the model
model = Sequential()

# First LSTM layer with return sequences
model.add(LSTM(lstm1_units,
               return_sequences=True,
               input_shape=(x_train.shape[1], 1),
               kernel_regularizer=l2(l2_reg)))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))

# Second LSTM layer
model.add(LSTM(lstm2_units,
               return_sequences=False,
               kernel_regularizer=l2(l2_reg)))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate))

# Dense layers
model.add(Dense(dense1_units, activation='relu'))
model.add(Dense(dense2_units, activation='relu'))
model.add(Dense(1))  # Output layer

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Display model summary
print("Model summary:")
model.summary()

"""### **5.2 Model Training Configuration**

Setting up training parameters and callbacks for optimal model performance.
"""

# Training parameters
batch_size = 32
epochs = 50
validation_split = 0.2

# Configure callbacks
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True,
        verbose=1
    ),
    ModelCheckpoint(
        'best_stock_model.h5',
        save_best_only=True,
        monitor='val_loss',
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.1,
        patience=3,
        verbose=1
    )
]

"""#### **5.3 Model Training**

Training the LSTM model with the prepared data and monitoring performance.
"""

# Train the model
history = model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=validation_split,
    callbacks=callbacks,
    verbose=1
)

# Plot training history
plt.figure(figsize=(25, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss During Training')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
epochs_trained = len(history.history['loss'])
plt.plot(range(1, epochs_trained + 1), history.history['loss'], 'bo-', markersize=4)
plt.plot(range(1, epochs_trained + 1), history.history['val_loss'], 'ro-', markersize=4)
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Training', 'Validation'])
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()



print(f"Final training loss: {history.history['loss'][-1]:.6f}")
print(f"Final validation loss: {history.history['val_loss'][-1]:.6f}")

"""## **6. Model Evaluation**

### **6.1 Making Predictions**

Using the trained model to make predictions on the test set and converting back to original price scale.
"""

# Make predictions on test set
predictions = model.predict(x_test, batch_size=batch_size)
print(f"Predictions shape: {predictions.shape}")

# Transform predictions back to original scale
inv_predictions = scaler.inverse_transform(predictions.reshape(-1, 1))
inv_y_test = scaler.inverse_transform(y_test.reshape(-1, 1))
print(f"Predictions range: ${inv_predictions.min():.2f} - ${inv_predictions.max():.2f}")
print(f"Actual prices range: ${inv_y_test.min():.2f} - ${inv_y_test.max():.2f}")

"""### **6.2 Performance Metrics**

Calculating comprehensive evaluation metrics to assess model performance.
"""

# Calculate error metrics
rmse = np.sqrt(np.mean((inv_predictions - inv_y_test)**2))
mae = mean_absolute_error(inv_y_test, inv_predictions)
mape = mean_absolute_percentage_error(inv_y_test, inv_predictions)
r2 = r2_score(inv_y_test, inv_predictions)

# Display metrics
print(f"- Root Mean Square Error (RMSE): ${rmse:.2f}")
print(f"- Mean Absolute Error (MAE): ${mae:.2f}")
print(f"- Mean Absolute Percentage Error (MAPE): {mape:.2%}")
print(f"- RÂ² Score: {r2:.4f}")

# Calculate directional accuracy
actual_direction = np.sign(np.diff(inv_y_test.flatten()))
pred_direction = np.sign(np.diff(inv_predictions.flatten()))
directional_accuracy = np.mean(actual_direction == pred_direction)
print(f"- Directional Accuracy: {directional_accuracy:.2%}")

# Error distribution analysis
errors = inv_predictions.flatten() - inv_y_test.flatten()
print(f"- Mean Error: ${np.mean(errors):.2f}")
print(f"- Error Std Dev: ${np.std(errors):.2f}")
print(f"- Max Positive Error: ${np.max(errors):.2f}")
print(f"- Max Negative Error: ${np.min(errors):.2f}")

"""### **6.3 Visualization of Results**

Creating comprehensive visualizations to assess model performance and prediction quality.
"""

# Create results dataframe
results_df = pd.DataFrame({
    'Actual': inv_y_test.flatten(),
    'Predicted': inv_predictions.flatten(),
    'Error': errors,
    'Abs_Error': np.abs(errors)
}, index=google_data.index[splitting_len + lookback_window:])

plt.figure(figsize=(25, 10))
plt.subplot(2, 2, 1)
plt.plot(results_df.index, results_df['Actual'], label='Actual Price', linewidth=2, alpha=0.8)
plt.plot(results_df.index, results_df['Predicted'], label='Predicted Price', linestyle='--', linewidth=2)
plt.fill_between(results_df.index,
                 results_df['Actual'] - results_df['Abs_Error'],
                 results_df['Actual'] + results_df['Abs_Error'],
                 alpha=0.2, label='Error Range')
plt.title('Stock Price Prediction vs Actual')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. Prediction errors over time
plt.subplot(2, 2, 2)
plt.plot(results_df.index, results_df['Error'], color='red', alpha=0.7)
plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)
plt.title('Prediction Errors Over Time')
plt.xlabel('Date')
plt.ylabel('Error ($)')
plt.grid(True, alpha=0.3)

# 3. Error distribution
plt.subplot(2, 2, 3)
plt.hist(results_df['Error'], bins=30, alpha=0.7, edgecolor='black', color='skyblue')
plt.axvline(np.mean(results_df['Error']), color='red', linestyle='--', label=f'Mean: ${np.mean(results_df["Error"]):.2f}')
plt.title('Error Distribution')
plt.xlabel('Prediction Error ($)')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)

# 4. Actual vs Predicted scatter plot
plt.subplot(2, 2, 4)
plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.6, s=20)
plt.plot([results_df['Actual'].min(), results_df['Actual'].max()],
         [results_df['Actual'].min(), results_df['Actual'].max()],
         'r--', linewidth=2, label='Perfect Prediction')
plt.xlabel('Actual Price ($)')
plt.ylabel('Predicted Price ($)')
plt.title('Actual vs Predicted Scatter Plot')
plt.legend()
plt.grid(True, alpha=0.3)







# Focus on last 100 predictions for detailed analysis
recent_results = results_df.tail(100)

plt.figure(figsize=(25, 10))
plt.subplot(2, 1, 1)
plt.plot(recent_results.index, recent_results['Actual'], 'o-', label='Actual Price', linewidth=2, markersize=4)
plt.plot(recent_results.index, recent_results['Predicted'], 's-', label='Predicted Price', linewidth=2, markersize=4)
plt.title('Recent Predictions (Last 100 Trading Days)')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(2, 1, 2)
plt.bar(recent_results.index, recent_results['Error'], alpha=0.7, width=2)
plt.axhline(y=0, color='black', linestyle='-', alpha=0.8)
plt.title('Recent Prediction Errors')
plt.xlabel('Date')
plt.ylabel('Error ($)')
plt.grid(True, alpha=0.3)



# Performance summary
print(f"Model trained on {len(x_train)} sequences")
print(f"Model tested on {len(x_test)} sequences")
print(f"Average daily prediction error: ${mae:.2f}")
print(f"Prediction accuracy (MAPE): {(1-mape)*100:.1f}%")
print(f"Direction prediction accuracy: {directional_accuracy*100:.1f}%")

"""### **6.4 Model Saving**

Saving the trained model for future use and deployment.
"""

# Saving the final model
model.save("google_stock_price_prediction_model.keras")

# Save the scaler for future predictions
joblib.dump(scaler, 'stock_price_scaler.pkl')

# from google.colab import files
# files.download('google_stock_price_prediction_model.keras')
# files.download('stock_price_scaler.pkl')

"""#### Conclusion

The project developed an LSTM-based model using 20 years of Google stock data, achieving under 2% MAPE and 70% directional accuracy. RMSE and MAE results confirm its reliability for practical forecasting. Analysis showed a long-term bullish trend with volatility spikes during the 2008 crisis and 2020 pandemic. Moving averages and volatility metrics provided signals for trend shifts and reversals. The model supports investment, risk management and trading strategies, with strong adaptation to recent market conditions.

Up next we will create a Streamlit app for ease usability.

"""